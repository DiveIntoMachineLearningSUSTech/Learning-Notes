## Gauss Mixture Models and Expectation Maxmium

* [<font size="">The Multivariate Gaussian Distribution</font>](http://cs229.stanford.edu/section/gaussians.pdf)

* Chapter 9 of PRML

------------------------

## Genetic algorithm

* [Ai junki](http://www.ai-junkie.com/ga/intro/gat1.html)

    the key precedoure is find a way deciding the fitness score of each potential solution.

------------------------

## Neural Network

* [link1](http://neuralnetworksanddeeplearning.com/chap1.html#complete_zero) (a little tedious but very detailed)

* [link2](https://www.zybuluo.com/hanbingtao/note/476663)(TODO)

* [Coursera - ML by Andrew Ng](https://www.coursera.org/learn/machine-learning?authMode=login)

* [Cross-entropy](http://colah.github.io/posts/2015-09-Visual-Information/)

    >We call this difference the `Kullback–Leibler divergence`, or just the `KL` divergence. The KL divergence of $P$ with respect to $Q$, $D_Q(P)$ is defined:<br>
    $D_Q(P)=H_Q(P) - H_P(Q)$<br>
    The really neat thing about KL divergence is that it’s like a distance between two distributions. It measures how different they are!

    [chinese version](http://studyai.site/2017/06/13/%E3%80%90%E7%BF%BB%E8%AF%91%E3%80%91%E8%A7%86%E8%A7%89%E4%BF%A1%E6%81%AF%E8%AE%BA/)

------------------------