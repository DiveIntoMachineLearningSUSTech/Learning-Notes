## Gauss Mixture Models and Expectation Maxmium

* [<font size="">The Multivariate Gaussian Distribution</font>](http://cs229.stanford.edu/section/gaussians.pdf)

* Chapter 9 of PRML

------------------------

## Genetic algorithm

* [Ai junki](http://www.ai-junkie.com/ga/intro/gat1.html)

    the key precedoure is find a way deciding the fitness score of each potential solution.

------------------------

## Neural Network

* [link1](http://neuralnetworksanddeeplearning.com/chap1.html#complete_zero) (a little tedious but very detailed)

* [link2](https://www.zybuluo.com/hanbingtao/note/476663)(TODO)

* [Coursera - ML by Andrew Ng](https://www.coursera.org/learn/machine-learning?authMode=login)

* [Cross-entropy](http://colah.github.io/posts/2015-09-Visual-Information/)

    >We call this difference the `Kullback–Leibler divergence`, or just the `KL` divergence. The KL divergence of $P$ with respect to $Q$, $D_Q(P)$ is defined:<br>
    $D_Q(P)=H_Q(P) - H(P)$<br>
    The really neat thing about KL divergence is that it’s like a distance between two distributions. It measures how different they are!

    [chinese version](http://studyai.site/2017/06/13/%E3%80%90%E7%BF%BB%E8%AF%91%E3%80%91%E8%A7%86%E8%A7%89%E4%BF%A1%E6%81%AF%E8%AE%BA/)

------------------------

## PageRank 

[PageRank Algorithm - The Mathematics of Google Search](http://www.math.cornell.edu/~mec/Winter2009/RalucaRemus/Lecture3/lecture3.html) (**_read recommended strongly!_**) perfectly explained the insight and intuition of PageRank algorithm, and why the iterative process will eventually convergence!

------------------------

## Softmax and Sigmoid Regression

[UFLDL Home](http://ufldl.stanford.edu/tutorial/)  
[UFLDL Sigmoid/Logistic Regression](http://ufldl.stanford.edu/tutorial/supervised/LogisticRegression/)  
[UFLDL Softmax Regression](http://ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression/)  ([Chinese version](http://ufldl.stanford.edu/wiki/index.php/Softmax%E5%9B%9E%E5%BD%92#.E4.B8.AD.E6.96.87.E8.AF.91.E8.80.85))

-------------------------

## CART

[Regression Tree](https://cethik.vip/2016/09/21/machineCAST/)

-------------------------

## Support Vector Machine

* [A Python online book](https://jakevdp.github.io/PythonDataScienceHandbook/)
    
    there is one chapter about how to use **_svm_** to classify data set using `scikit-learn`

* [Lecture cs.columbia.edu](http://www.cs.columbia.edu/~kathy/cs4701/documents/jason_svm_tutorial.pdf)
* [Lecture cs229.stanford.edu](http://cs229.stanford.edu/notes/cs229-notes3.pdf)
* 【机器学习 周志华】相关章节

-----------------------

## Naive Bayes Classifier

* [Stanford NLP Lecture](https://web.stanford.edu/class/cs124/lec/naivebayes.pdf)  
    mainly target for nlp

* [CMU ML Lecture](https://www.cs.cmu.edu/~tom/mlbook/NBayesLogReg.pdf)