## Gauss Mixture Models and Expectation Maxmium

* [<font size="">The Multivariate Gaussian Distribution</font>](http://cs229.stanford.edu/section/gaussians.pdf)

* Chapter 9 of PRML

------------------------

## Genetic algorithm

* [Ai junki](http://www.ai-junkie.com/ga/intro/gat1.html)

    the key precedoure is find a way deciding the fitness score of each potential solution.

------------------------

## Neural Network

* [link1](http://neuralnetworksanddeeplearning.com/chap1.html#complete_zero) (a little tedious but very detailed)

* [link2](https://www.zybuluo.com/hanbingtao/note/476663)(TODO)

* [Coursera - ML by Andrew Ng](https://www.coursera.org/learn/machine-learning?authMode=login)

* [Cross-entropy](http://colah.github.io/posts/2015-09-Visual-Information/)

    >We call this difference the `Kullback–Leibler divergence`, or just the `KL` divergence. The KL divergence of $P$ with respect to $Q$, $D_Q(P)$ is defined:<br>
    $D_Q(P)=H_Q(P) - H(P)$<br>
    The really neat thing about KL divergence is that it’s like a distance between two distributions. It measures how different they are!

    [chinese version](http://studyai.site/2017/06/13/%E3%80%90%E7%BF%BB%E8%AF%91%E3%80%91%E8%A7%86%E8%A7%89%E4%BF%A1%E6%81%AF%E8%AE%BA/)

------------------------

## PageRank 

[PageRank Algorithm - The Mathematics of Google Search](http://www.math.cornell.edu/~mec/Winter2009/RalucaRemus/Lecture3/lecture3.html) (**_read recommended strongly!_**) perfectly explained the insight and intuition of PageRank algorithm, and why the iterative process will eventually convergence!

------------------------

## Softmax and Sigmoid Regression

[UFLDL Home](http://ufldl.stanford.edu/tutorial/)  
[UFLDL Sigmoid/Logistic Regression](http://ufldl.stanford.edu/tutorial/supervised/LogisticRegression/)  
[UFLDL Softmax Regression](http://ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression/)  ([Chinese version](http://ufldl.stanford.edu/wiki/index.php/Softmax%E5%9B%9E%E5%BD%92#.E4.B8.AD.E6.96.87.E8.AF.91.E8.80.85))